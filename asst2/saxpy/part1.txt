Question 1.
What performance do you observe compared to the sequential CPU-based implementation of SAXPY (recall program 3 from Assignment 1)? Compare and explain the difference between the results provided by two sets of timers (the timer you added and the timer that was already in the provided starter code). Are the bandwidth values observed consistent with the peak bandwidths available to the different components of the machine?

We got the following output from our parallelized saxpy implementation:
Overall: 23.518 ms              [4.752 GB/s]
Kernel : 0.933 ms               [119.814 GB/s]
Overall: 22.796 ms              [4.902 GB/s]
Kernel : 0.934 ms               [119.668 GB/s]
Overall: 22.185 ms              [5.037 GB/s]
Kernel : 0.935 ms               [119.578 GB/s]

The timer measuring the overall runtime of the program (already present) shows a much greater time than the
 timer we added measuring the runtime of the kernel. This is because the kernel runs in a highly parallel m
anner over the input arrays, where each thread only executes a few instructions on a single index. In the best case where every thread is executing at once, this results in a constant runtime O(1). The main CUDA program, however, performs a copy of two extremely large arrays from host memory to device memory, and then afer the kernel completes copies the extremely large results array back into host memory. These memory copy operations are carried out sequentially and in the best case is an O(N) operation. Thus the overall time is much longer than the kernel time.

In addition, the performance of the program is constrained by the available bandwidth. Most significantly,
the memory copy operations performed by the main CUDA program are moving data from main memory to the GPU m
emory over the PCIe bus, which is not very high-bandwidth. As shown by our bandwidth calculations the PCIe
bus taps out at around 5 GB/s, and is probably a limiting factor in the performance of the array copy opera
tions along with the sequential implementation. On the other hand, the thread operations (which involve mov
ing data between core memory and the device memory) are operating at a bandwidth at about 120 GB/s, which i
s not quite as fast as the GPU's maximum device memory bandwidth of 177 GB/s. This implies that bandwidth i
s not a limiting factor for the kernel section's performance, although it is certainly not being underutili
zed.

